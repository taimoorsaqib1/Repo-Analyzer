# Repo Analyzer â€” AI Coding Assistant

An AI-powered coding assistant that lets you **chat with any GitHub repository** using Retrieval-Augmented Generation (RAG). Add one or more repo URLs and it clones, indexes, and analyses them â€” then lets you ask questions through a **web UI** or a **terminal CLI**.

---

## Features

### Core
- **Multi-repo support** â€” analyze several repositories in a single session
- **Dual provider** â€” use **OpenAI** or a **local Ollama** model (fully offline)
- **Language-aware chunking** â€” code split at function/class boundaries with contextual headers
- **Real-time streaming** â€” tokens stream in both the web app and the CLI
- **Conversation memory** â€” multi-turn questions with full history
- **Source file references** â€” every answer links to the exact files used
- Supports 25+ file types: Python, JS/TS, Go, Rust, Java, C/C++, HTML, CSS, SQL, and more

### Phase 1 â€” Retrieval Quality
- **Hybrid Search (BM25 + MMR + RRF)** â€” keyword and semantic search merged via Reciprocal Rank Fusion
- **Contextual Retrieval** â€” each chunk prefixed with a file-level context header for better embedding quality
- **Cross-encoder Re-ranking** â€” top results re-scored by `BAAI/bge-reranker-base` before passing to the LLM

### Phase 2 â€” UI & Persistence
- **3-tab Web App** â€” Overview Â· Explorer Â· Chat
- **Workspace persistence** â€” save and reload named sessions (repos + vector collection) from a local SQLite registry
- **File Explorer** â€” browse the indexed file tree of any ingested repo
- **Code Viewer** â€” click any file to read it with syntax highlighting (50+ languages via highlight.js)
- **Markdown chat** â€” answers rendered as formatted Markdown with code blocks
- **Source chips** â€” clickable file references jump straight to the Code Viewer
- **Auto-generated repo summaries** â€” purpose, features, use cases, tech stack, architecture, entry points, getting started, and limitations

---

## Project Structure

```
Repo-Analyzer/
â”œâ”€â”€ core/                   # All RAG logic (importable package)
â”‚   â”œâ”€â”€ assistant.py        # RAG chain â€” retriever + cross-encoder re-ranker + LLM + streaming
â”‚   â”œâ”€â”€ chunker.py          # Language-aware code splitter with contextual headers
â”‚   â”œâ”€â”€ config.py           # Centralized config (reads .env)
â”‚   â”œâ”€â”€ embeddings.py       # Embedding model factory (OpenAI / Ollama)
â”‚   â”œâ”€â”€ llm.py              # LLM factory (OpenAI / Ollama)
â”‚   â”œâ”€â”€ loader.py           # File loader + multi-repo Git clone helper
â”‚   â”œâ”€â”€ retriever.py        # Hybrid BM25 + MMR retriever with Reciprocal Rank Fusion
â”‚   â”œâ”€â”€ vectorstore.py      # ChromaDB create / load / clear helpers (named collections)
â”‚   â””â”€â”€ workspace.py        # SQLite workspace registry (save / load / list / delete)
â”œâ”€â”€ web/                    # Web application
â”‚   â”œâ”€â”€ server.py           # FastAPI backend â€” ingest, chat, workspaces, file tree, file content
â”‚   â””â”€â”€ static/
â”‚       â”œâ”€â”€ index.html      # 3-tab SPA (Overview / Explorer / Chat)
â”‚       â”œâ”€â”€ style.css       # Dark design system (~1000 lines)
â”‚       â””â”€â”€ app.js          # Frontend logic â€” workspaces, file tree, code viewer, markdown chat
â”œâ”€â”€ data/                   # Runtime data (auto-created, git-ignored)
â”‚   â”œâ”€â”€ chroma_db/          # Vector database (per-session UUID collections)
â”‚   â”œâ”€â”€ repo_clone/         # Cloned repos
â”‚   â””â”€â”€ workspaces.db       # SQLite workspace registry
â”œâ”€â”€ cli.py                  # Terminal chat interface
â”œâ”€â”€ ingest.py               # Ingestion CLI (local paths or Git URLs)
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env                    # Your config (copy from .env.example)
```

---

## Quick Start

### 1. Clone this repo

```bash
git clone https://github.com/taimoorsaqib1/Repo-Analyzer.git
cd Repo-Analyzer
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

### 3. Configure environment

Create a `.env` file in the project root:

```env
PROVIDER=ollama
OLLAMA_LLM_MODEL=qwen2.5-coder:7b
OLLAMA_EMBED_MODEL=nomic-embed-text
```

---

## Option A â€” Web App (Recommended)

```bash
python web/server.py
```

Open **http://localhost:8000** in your browser.

### Overview Tab
1. Paste one or more GitHub repo URLs in the sidebar
2. Set the branch (default: `main`) and click **Analyze Repos**
3. The app clones, indexes, and generates rich overview cards with purpose, features, tech stack, architecture, entry points, and more

### Explorer Tab
- Browse the full file tree of every indexed repo
- Click any file to open it in the **Code Viewer** panel with syntax highlighting

### Chat Tab
- Ask natural-language questions about the code
- Answers are fully Markdown-rendered with code blocks and syntax highlighting
- Source chips below each answer are clickable â€” they open the referenced file in the Code Viewer

### Workspaces
- Click **Save Session** to persist the current repos and vector collection under a name
- Saved workspaces appear in the sidebar â€” click to reload any previous session instantly
- Workspaces are stored in `data/workspaces.db` (SQLite, local)

---

## Option B â€” Terminal CLI

### Index repositories

```bash
# Single remote repo
python ingest.py --git https://github.com/owner/repo

# Multiple remote repos
python ingest.py --git https://github.com/user/repo1 https://github.com/user/repo2

# Specific branch
python ingest.py --git https://github.com/owner/repo --branch develop

# Local folders
python ingest.py /path/to/repo1 /path/to/repo2

# Force re-index
python ingest.py --git https://github.com/owner/repo --force
```

### Chat

```bash
python cli.py
```

#### CLI Commands

| Command | Description |
|---|---|
| `/help` | Show available commands |
| `/clear` | Clear conversation history |
| `/sources` | Show source files from the last answer |
| `/config` | Show current configuration |
| `/quit` | Exit and clean up session data |

---

## Configuration

| Variable | Default | Description |
|---|---|---|
| `PROVIDER` | `ollama` | `openai` or `ollama` |
| `OPENAI_API_KEY` | â€” | Your OpenAI API key |
| `OPENAI_EMBED_MODEL` | `text-embedding-3-small` | OpenAI embedding model |
| `OPENAI_LLM_MODEL` | `gpt-4` | OpenAI chat model |
| `OLLAMA_BASE_URL` | `http://localhost:11434` | Ollama server URL |
| `OLLAMA_EMBED_MODEL` | `nomic-embed-text` | Ollama embedding model |
| `OLLAMA_LLM_MODEL` | `qwen2.5-coder:7b` | Ollama chat model |
| `CHROMA_PERSIST_DIR` | `./chroma_db` | Vector database location |
| `REPO_CLONE_DIR` | `./data/repo_clone` | Cloned repos location |
| `CHUNK_SIZE` | `1500` | Tokens per chunk |
| `CHUNK_OVERLAP` | `200` | Overlap between chunks |
| `RETRIEVER_K` | `6` | Chunks retrieved per query |

### Using Ollama (local, free, offline)

```bash
# Install Ollama from https://ollama.com, then:
ollama pull qwen2.5-coder:7b   # LLM (recommended for 6 GB+ VRAM)
ollama pull nomic-embed-text   # Embeddings
```

Set `PROVIDER=ollama` in `.env`.

### Using OpenAI

Set `PROVIDER=openai` and `OPENAI_API_KEY=sk-...` in `.env`.

---

## Requirements

- Python 3.10+
- [Ollama](https://ollama.com) *(if using local models)*
- OpenAI API key *(if using OpenAI)*

Key Python packages (see `requirements.txt` for full list):

| Package | Purpose |
|---|---|
| `langchain` / `langchain-community` | RAG chain, document loaders |
| `langchain-ollama` | Ollama LLM + embeddings integration |
| `langchain-openai` | OpenAI LLM + embeddings integration |
| `langchain-chroma` | ChromaDB vector store integration |
| `chromadb` | Persistent vector database |
| `rank_bm25` | BM25 keyword retriever (Phase 1) |
| `sentence-transformers` | Cross-encoder re-ranker â€” `BAAI/bge-reranker-base` (Phase 1) |
| `fastapi` + `uvicorn` | Web server |
| `gitpython` | Git clone support |

---

## License

MIT

- **Web App** â€” modern dark UI with repo overview cards and streaming chat
- **Auto-generated repo summaries** â€” purpose, features, use cases, tech stack, architecture, entry points, getting started, and limitations
- **Interactive CLI** â€” full-featured terminal chat with rich formatting
- **Dual provider** â€” use **OpenAI** or a **local Ollama** model (fully offline)
- **Language-aware chunking** â€” code split at function/class boundaries
- **Real-time streaming** â€” tokens stream in the web app and CLI
- **Session cleanup** â€” ChromaDB and cloned repos wiped automatically on exit
- Supports 25+ file types: Python, JS/TS, Go, Rust, Java, C/C++, HTML, CSS, SQL, and more
- Conversation memory across multi-turn questions
- Source file references shown for every answer

---

## Project Structure

```
Repo-Analyzer/
â”œâ”€â”€ core/                   # All RAG logic (importable package)
â”‚   â”œâ”€â”€ assistant.py        # RAG chain â€” retriever + LLM + prompt + streaming
â”‚   â”œâ”€â”€ chunker.py          # Language-aware code splitter
â”‚   â”œâ”€â”€ config.py           # Centralized config (reads .env)
â”‚   â”œâ”€â”€ embeddings.py       # Embedding model factory (OpenAI / Ollama)
â”‚   â”œâ”€â”€ llm.py              # LLM factory (OpenAI / Ollama)
â”‚   â”œâ”€â”€ loader.py           # File loader + multi-repo Git clone helper
â”‚   â”œâ”€â”€ retriever.py        # ChromaDB MMR retriever
â”‚   â””â”€â”€ vectorstore.py      # ChromaDB create / load / clear helpers
â”œâ”€â”€ web/                    # Web application
â”‚   â”œâ”€â”€ server.py           # FastAPI backend with SSE streaming
â”‚   â””â”€â”€ static/
â”‚       â”œâ”€â”€ index.html      # Single-page app
â”‚       â”œâ”€â”€ style.css       # Dark theme UI
â”‚       â””â”€â”€ app.js          # Frontend logic (no framework)
â”œâ”€â”€ data/                   # Runtime data (auto-created, auto-deleted)
â”‚   â”œâ”€â”€ chroma_db/          # Vector database
â”‚   â””â”€â”€ repo_clone/         # Cloned repos, one folder per repo
â”œâ”€â”€ cli.py                  # Terminal chat interface
â”œâ”€â”€ ingest.py               # Ingestion CLI (local paths or Git URLs)
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env                    # Your config
```

---

## Quick Start

### 1. Clone this repo

```bash
git clone https://github.com/taimoorsaqib1/Repo-Analyzer.git
cd Repo-Analyzer
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

### 3. Configure environment

Create a `.env` file in the project root:

```env
PROVIDER=ollama
OLLAMA_LLM_MODEL=qwen2.5-coder:7b
OLLAMA_EMBED_MODEL=nomic-embed-text
```

---

## Option A â€” Web App (Recommended)

```bash
python web/server.py
```

Open **http://localhost:8000** in your browser.

1. Paste one or more GitHub repo URLs in the sidebar
2. Set the branch (default: `main`) and click **Analyze Repos**
3. The app clones, indexes, and generates rich overview cards:
   - ğŸ¯ Purpose Â· âœ¨ Key Features Â· ğŸ’¡ Use Cases
   - ğŸ›  Tech Stack Â· ğŸ”— External Dependencies Â· ğŸ“Š Languages
   - ğŸ— Architecture Â· ğŸšª Entry Points Â· ğŸš€ Getting Started Â· âš  Limitations
4. Switch to the **Chat** tab to ask questions in real time
5. Click **Clear Session** â€” everything is wiped automatically

---

## Option B â€” Terminal CLI

### Index repositories

```bash
# Single remote repo
python ingest.py --git https://github.com/owner/repo

# Multiple remote repos
python ingest.py --git https://github.com/user/repo1 https://github.com/user/repo2

# Specific branch
python ingest.py --git https://github.com/owner/repo --branch develop

# Local folders
python ingest.py /path/to/repo1 /path/to/repo2

# Force re-index
python ingest.py --git https://github.com/owner/repo --force
```

### Chat

```bash
python cli.py
```

#### CLI Commands

| Command | Description |
|---|---|
| `/help` | Show available commands |
| `/clear` | Clear conversation history |
| `/sources` | Show source files from the last answer |
| `/config` | Show current configuration |
| `/quit` | Exit and clean up session data |

---

## Configuration

| Variable | Default | Description |
|---|---|---|
| `PROVIDER` | `ollama` | `openai` or `ollama` |
| `OPENAI_API_KEY` | â€” | Your OpenAI API key |
| `OPENAI_EMBED_MODEL` | `text-embedding-3-small` | OpenAI embedding model |
| `OPENAI_LLM_MODEL` | `gpt-4` | OpenAI chat model |
| `OLLAMA_BASE_URL` | `http://localhost:11434` | Ollama server URL |
| `OLLAMA_EMBED_MODEL` | `nomic-embed-text` | Ollama embedding model |
| `OLLAMA_LLM_MODEL` | `qwen2.5-coder:7b` | Ollama chat model |
| `CHROMA_PERSIST_DIR` | `./data/chroma_db` | Vector database location |
| `REPO_CLONE_DIR` | `./data/repo_clone` | Cloned repos location |
| `CHUNK_SIZE` | `1500` | Tokens per chunk |
| `CHUNK_OVERLAP` | `200` | Overlap between chunks |
| `RETRIEVER_K` | `6` | Chunks retrieved per query |

### Using Ollama (local, free, offline)

```bash
# Install Ollama from https://ollama.com, then:
ollama pull qwen2.5-coder:7b   # LLM (recommended for 6GB+ VRAM)
ollama pull nomic-embed-text   # Embeddings
```

Set `PROVIDER=ollama` in `.env`.

### Using OpenAI

Set `PROVIDER=openai` and `OPENAI_API_KEY=sk-...` in `.env`.

---

## Requirements

- Python 3.10+
- [Ollama](https://ollama.com) (if using local models)
- OpenAI API key (if using OpenAI)

---

## License

MIT
